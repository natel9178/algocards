const terms = {
  "part of speech pos tagging":
    "Part-of-Speech tagging is the process of reading\nnatural language text and assigning parts of speech to each\ntoken.\n\nOne could imagine taking in a sentence like:\n\n> The dog ran away.\n\nand creating a data structure that had the following annotations:\n\n> The*[article]* dog*[noun]* ran*[verb]* away*[adjective]*.\n\nWords can have different parts-of-speech depending on their\ncontext. For example, the word *away* can be either an [adverb\nor an adjective, or part of a larger phrase][1].\n\n[1]: http://www.dictionary.com/browse/away",
  entailment:
    'The word *entailment* has many meanings. Please review Wikipedia\'s [disambiguation page for "entail"][1]\nfor definitions that do not refer to *textual entailment*.\n\nTextual entailment refers to drawing a relation from an *entailing text*\nto an *entailed hypothesis*.\n\nThe Wikipedia page for [textual entailment][2] gives the following examples:\n\n> Textual entailment can be illustrated with examples of three different relations:\n>\n> An example of a positive TE (text entails hypothesis) is:\n>\n>  - **text**: If you help the needy, God will reward you.\n>  - **hypothesis**: Giving money to a poor man has good consequences.\n>\n> An example of a negative TE (text contradicts hypothesis) is:\n>\n>  - **text**: If you help the needy, God will reward you.\n>  - **hypothesis**: Giving money to a poor man has no consequences.\n>\n> An example of a non-TE (text does not entail nor contradict) is:\n>\n> - **text**: If you help the needy, God will reward you.\n> - **hypothesis**: Giving money to a poor man will make you a better person.\n\n[1]: https://en.wikipedia.org/wiki/Entail_(disambiguation)\n[2]: https://machinelearning.wtf/terms/textual-entailment/',
  syntaxnet:
    "SyntaxNet is a framework for natural language syntactic\nparsers released by Google in 2016.\n\nSyntaxNet tags words in a sentence with their syntactic part-of-speech\nand creates a parse tree showing dependencies between words\nin a sentence.\n\nParsey McParseface is a SyntaxNet model trained on the English\nlanguage. At its time of release, Parsey McParseface is the\nworld's most accurate model of its kind.",
  "face verification":
    "*Face verification* is the problem of identifying\nwhether an image belongs to a person--given\nthe one image and one person as the input.\n\nFace verification is an easier problem than\n[face recognition][1] because face verification only compares\na single image to one person, whereas face recognition does this\nfor $k$ people.\n\n[1]: https://machinelearning.wtf/terms/face-recognition/",
  "gram matrix":
    "[Wolfram Mathworld defines Gram matrix as][1]:\n\n> Given a set $V$ of $m$ vectors (points in $\\mathcal R^n$), the Gram matrix $G$ is\n> the matrix of all possible inner products of $V$, i.e.,\n> $$ g_{ij} = \\mathbf v_i^T \\mathbf v_j $$\n\n[1]: http://mathworld.wolfram.com/GramMatrix.html",
  unk:
    "UNK, unk, `<unk>` are variants of a symbol in natural language processing\nand machine translation to indicate an out-of-vocabulary word.\n\nMany language models do calculations upon representations of\nthe $n$ most frequent words in the corpus. Words that are less\nfrequent are replaced with the `<unk>` symbol.\n\nThis is what such a transformation might look like. The below\nis an example of a source document in a corpus with\ncommon English words.\n\n\n> Today I'll bake; tomorrow I'll brew, \n> Then I'll fetch the queen's new child, \n> It is good that no one knows, \n> **Rumpelstiltskin** is my name.\n\nEvery word in the above quote is common in English, except for\nRumpelstiltskin, which is replaced as following:\n\n> Today I'll bake; tomorrow I'll brew, \n> Then I'll fetch the queen's new child, \n> It is good that no one knows, \n> **&lt;unk&gt;** is my name.",
  "abstractive sentence summarization":
    "Abstractive sentence summarization refers to creating a shorter version of a\nsentence with the same meaning.\n\nThis is in contrast to extractive sentence summarization, which pulls the\nmost informative sentences from a document.",
  "extractive sentence summarization":
    'Extractive sentence summarization refers to programmatically\ncreating a shorter version of a document by extracting\nthe "important" parts.\n\n[TextRank][1] is an example of an algorithm that can\nrank sentences in a document for the purpose of extractive\nsummarization.\n\n[1]: https://machinelearning.wtf/terms/textrank',
  "neural turing machine ntm":
    "Neural Turing Machines (NTM) consists of a RNN (commonly with LSTM), and a memory bank, where the neural network can make write and read operations. By making each operation of the NTM differentiable, it can be trained efficiently trained with gradient descent.\n\nThe main idea of the NTM is to use the memory bank -- a large, addressable memory -- to give a memory to the RNN so that it can read and write to, yielding a practical mechanism to learn programs. The NTM has been shown to be able to infer simple algorithms, such as copying, sorting and associative recall from input and output examples.",
  "padding convolution":
    "Padding is a preprocessing step before a convolution operation.\n\nWhen we [convolve][1] a $n \\times n$ image with an $f \\times f$ filter\nand a stride length of $1$,\nthe output is a matrix of dimension $n - f \\times n - f$.\n\nFor deep convolutional neural networks that may do many convolutions,\nthis would cause the input matrix to dramatically shrink and become\ntoo small.\n\nAdditionally, values in the middle of the input matrix have a greater\ninfluence on the output than values on the edges.\n\nThere are several different methods for choosing what values to pad an input\nmatrix with:\n\n - [Zero-padding][2] -- padding with zeroes\n - Repeating the nearest border values as values for padding\n - Using values from the opposite side of the matrix as padding values\n\n[1]: https://machinelearning.wtf/terms/convolution/\n[2]: https://machinelearning.wtf/terms/zero-pad/",
  "deep convolutional generative adversarial network dcgan":
    "DCGAN refers to a model described by [Radford, Metz, and Chintala][1]\nthat uses deep convolutional neural networks in a generative adversarial network model.\n\nGenerative adversarial networks (GANs) are structured as a competition between\ntwo models:\n\n1. a generative model that tries to create fake examples of training data interspersed with real training data.\n2. a discriminative model that tries to classify real examples from fake ones.\n\nDCGAN uses deep convolutional neural networks for both models. Convolutional neural networks (CNNs)\nare well-known for their performance on image data. DCGAN uses the strong performance of (CNNs)\nto learn [unsupervised representations][2] of the input data.\n\n[1]: https://arxiv.org/abs/1511.06434\n[2]: https://machinelearning.wtf/terms/unsupervised-learning/",
  burstiness:
    "Wikipedia defines burstiness as follows:\n\n> In statistics, burstiness is the intermittent increases and decreases in activity or frequency of an event. One of measures of burstiness is the Fano factor\u2014a ratio between the variance and mean of counts.\n\n## Word Burstiness\nIn natural language processing, burstiness has a slightly more specific definition,\n[defined by Slava Katz in the mid 1990s][2].\n\nThe authors of [Accounting for Burstiness in Topic Models][1] give the following succinct definition\nof burstiness:\n\n> Church and Gale (1995) note that real texts systematically exhibit this phenomenon: a word is more likely to occur again in a document if it has already appeared in the document. Importantly, the burstiness of a word and its semantic content are positively correlated; words that are more informative are also more bursty.\n\nAdditionally, burstiness also tells us that later appearances of a word are [less significant][3]\nthan the first appearance.\n\n> If a term is used once in a document, then it is likely to be used again. This phenomenon is called burstiness, and it implies that the second and later appearances of a word are less significant than the first appearance.\n\n[1]: https://web.stanford.edu/~gdoyle/papers/doyle-elkan-2009-icml-paper.pdf\n[2]: http://dl.acm.org/citation.cfm?id=974690\n[3]: http://cseweb.ucsd.edu/~elkan/perplexity.html",
  "out of core":
    "The term *out-of-core* typically refers to processing data that is too large\nto fit into a computer's main memory.\n\nTypically, when a dataset fits neatly into a computer's main memory,\nrandomly accessing sections of data has a (relatively) small performance\npenalty.\n\nWhen data must be stored in a medium like a large spinning hard drive\nor an external computer network, it becomes very expensive to randomly\nseek to an arbitrary section of data or to process the same data\nmultiple times.\n\nIn such a case, an out-of-core algorithm would try to access all relevant\ndata in one sequence.\n\nHowever, modern computers have a deep memory hierarchy, and replacing\nrandom access with sequential access can increase performance even\non datasets that fit within memory.",
  "internal covariate shift":
    "The term *interal covariate shift* comes from the paper\n[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift][1].\n\nThe authors' precise definition is:\n\n> We define Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetwork parameters during training.\n\nIn neural networks, the output of the first layer feeds into the second\nlayer, the output of the second layer feeds into the third, and so on.\nWhen the parameters of a layer change, so does the distribution\nof inputs to subsequent layers.\n\nThese shifts in input distributions\ncan be problematic for neural networks, especially deep neural\nnetworks that could have a large number of layers.\n\n[Batch normalization][2] is a method intended to mitigate internal covariate\nshift for neural networks.\n\n[1]: https://arxiv.org/abs/1502.03167\n[2]: https://machinelearning.wtf/terms/batch-normalization/",
  "coreference resolution":
    "The Stanford NLP group [defines coreference resolution][1] as:\n\n> Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.\n\nCoreference resolution should not be confused with\n[Named Entity Recognition](https://machinelearning.wtf/terms/named-entity-recognition-ner/), which is focused on labeling\nsequences of text that refer to entities--but not focused\non linking those entities together.\n\n[1]: https://nlp.stanford.edu/projects/coref.shtml",
  "connectionist temporal classification ctc":
    "*Connectionist Temporal Classification* is a term coined in a paper by\nGraves et al. titled [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks][1].\n\nIt refers to the use of [recurrent neural networks][2]\n(which is a form of [connectionism][3]) for the purpose of\nlabeling unsegmented data sequences (AKA [temporal classification][4]).\n\n[1]: http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf\n[2]: https://machinelearning.wtf/terms/recurrent-neural-network/\n[3]: https://machinelearning.wtf/terms/connectionism/\n[4]: https://machinelearning.wtf/terms/temporal-classification/",
  "hinge loss":
    "From the scikit-learn documentation, we get [the following definition][1]:\n\n> The hinge_loss function computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)\n\n[1]: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter",
  lenet:
    "LeNet was an early convolutional neural network proposed\nby Lecun et al in the paper\n[Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n\nLeNet was designed for handwriting recognition. Many modern\nconvolutional neural network architectures are inspired by LeNet.",
  "magnet loss":
    "*Magnet loss* is a type of loss function used in\n[distance metric learning][1] machine learning problems.\nIt was introduced in the paper\n[Metric Learning with Adaptive Density Discrimination][2].\nThe authors of the paper introduced magnet loss as an improvement\nover [triplet loss][3] and other loss functions designed\nto learn a distance metric.\n\nInstead of working on individuals, pairs, or triplets of data points,\nmagnet loss operates on entire regions of the embedding space that the\ndata points inhabit. Magnet loss models the distributions of different\nclasses in the embedding space and works to reduce the overlap\nbetween distributions.\n\n[1]: https://machinelearning.wtf/terms/distance-metric-learning/\n[2]: https://arxiv.org/abs/1511.05939\n[3]: https://machinelearning.wtf/terms/triplet-loss/",
  "continuous bag of words cbow":
    "Continuous Bag of Words refers to a algorithm\nthat predicts a target word from its\nsurrounding context.\n\nCBOW is one of the algorithms used for training\n[word2vec](https://machinelearning.wtf/terms/word2vec/) vectors.",
  "rectified linear unit relu":
    'A Rectified Linear Unit is a common name for a neuron (the "unit")\nwith an activation function of $f(x) = \\max(0,x)$.\n\nNeural networks built with ReLU have the following advantages:\n\n - [gradient][1] computation is simpler because the activation\n function is computationally similar than comparable activation\n functions like $\\tanh(x)$.\n - Neural networks with ReLU are less susceptible to\n the [vanishing gradient problem][2] but may suffer from\n the [dying ReLU problem][3].\n\n [1]: https://machinelearning.wtf/terms/gradient/\n [2]: https://machinelearning.wtf/terms/vanishing-gradient-problem/\n [3]: https://machinelearning.wtf/terms/dying-relu/',
  "hadamard product":
    "The Hadamard product refers to component-wise multiplication of the same dimension.\nThe $\\odot$ symbol is commonly used as the Hadamard product operator.\n\nHere is an example for the Hadamard product for a pair of $3 \\times 3$ matrices.\n\n$$\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \n\\end{bmatrix}\n\\odot\n\\begin{bmatrix}\nj & k & l \\\\\nm & n & o \\\\\np & q & r\n\\end{bmatrix}\n=\n\\begin{bmatrix}\naj & bk & cl \\\\\ndm & ne & fo \\\\\ngp & hq & ir\n\\end{bmatrix}\n$$",
  "same convolution":
    "A *same convolution* is a type of convolution where the output\nmatrix is of the same dimension as the input matrix.\n\nFor a $n \\times n$ input matrix $A$ and a $f \\times f$ filter matrix $F$,\nthe output of the convolution $A * F$ is of dimension\n$\\left \\lfloor \\frac{n + 2p - f}{s} \\right \\rfloor + 1 \\times \\left \\lfloor \\frac{n + 2p - f}{s} \\right \\rfloor + 1$\nwhere $s$ represents the stride length and\n$p$ represents the padding.\n\nIn a same convolution:\n\n - $s$ is typically set to $1$\n - $p$ is set to $\\frac{f - 1}{2}$\n - $f$ is an odd number\n\nThe result is that $A$ is padded to be $n + p \\times n + p$\nand $A * F$ becomes $n \\times n$ -- the same as the original\ndimensions of $A$.",
  adagrad:
    "AdaGrad is a gradient-descent based optimization algorithm. It automatically\ntunes the [learning rate][1] based on its observations of the data's geometry.\nAdaGrad is designed to perform well with datasets that have infrequently-occurring\nfeatures.\n\n[1]: https://machinelearning.wtf/terms/learning-rate/",
  "momentum optimization":
    "Momentum is commonly understood as a variation of [stochastic gradient descent][1],\nbut with one important difference: stochastic gradient descent can\nunnecessarily oscillate, and doesn't accelerate based on the shape of the\ncurve.\n\nIn contrast, momentum can dampen oscillations and accelerate convergence.\n\nMomentum was originally [proposed in 1964 by Boris T. Polyak][2].\n\n[1]: https://machinelearning.wtf/terms/stochastic-gradient-descent-sgd/\n[2]: https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods",
  "co adaptation":
    "In neural networks, co-adaptation refers to when different hidden\nunits in a neural networks have highly correlated behavior.\n\nIt is better for computational efficiency and the the model's ability\nto learn a general representation if hidden units can detect\nfeatures independently of each other.\n\nA few different regularization techniques aim at reducing\nco-adapatation--[dropout][1] being a notable one.\n\n[1]: https://machinelearning.wtf/terms/dropout/",
  "reinforcement learning":
    "Reinforcement learning is about learning from feedback (reinforcement) while learning 'on the job', i.e. learning by trying, rather than from labeled answer data.\nThis is how robots may learn, but is also used for playing games, a tight feedback loop through e.g. score may help give the algorithm an idea of what works well.",
  "hopfield network hn":
    "A Hopfield network (HN) is a type of [recurrent neural network][1](RNN). The HNs have only one layer, with each neuron connected to every other neuron: All neurons act as input and output.  The model of the network consists of a set of neurons and corresponding set of unit delays, forming a multiple loop feedback system. The output of a neuron is, via a unit delay element, sent to all of the others neurons in the HN, except itself.\n\nSome important properties of HNs are that all neurons have binary outputs (either 0, 1 or -1, 1), it isn't allowed a connection from a neuron to itself, the neurons are updated at random order (asynchronously), and the weights between the neurons are symmetric. This last property guarantees that the energy function decreases while following the activation function rules, and HNs are guaranteed to converge to a local minimum.\n\nHopfield networks are used for pattern recognition, and provides a model for understanding human memory. A HN is initially trained to store a number of patterns, and then it's able to recognize any of the learned patterns by exposure to only partial or even corrupted information about the pattern (it returns the closest pattern or the best guess). This property makes the Hopfield network a form of associative memory (or content-addressable memory). It's applications include image detection and recognition, enhancement of X-Ray images, medical images restoration, etc.\n\n[1]: https://machinelearning.wtf/terms/recurrent-neural-network/",
  "parameter budget":
    "A *parameter budget* refers to the idea of constraining the number\nof learnable parameters for a machine learning model. Some types\nof parameters are more useful for improving a model\nthan others, thus they should be prioritized in a model\nwith a restricted parameter budget.\n\nIn neural networks, deeper networks seem to work better when the parameter\nbudget is constrained.\n\nA related idea is the *computational budget*, but the budget for overall computation is not strictly tied to the number of parameters in a model.",
  "filter convolution":
    "A *filter* (also known as a *kernel*) is a small matrix\nused in convolution operations.\n\nConvolution filters are commonly used in image processing\nto modify images or extract features.\n\nThe dimensions of a convolution filter are typically small,\nodd, and square. For example, convolution filters are typically\n$3 \\times 3$ or $5 \\times 5$ matrices. Odd dimensions are\npreferred to even dimensions.",
  "search based software engineering sbse":
    "Search-based software engineering applies search and\noptimization techniques to software engineering problems.\n\n[LDADE][/terms/latent-dirichlet-allocation-differential-evolution-ldade] is an example of a system that applies search-based software engineering to optimize topic modeling.",
  "similarity learning":
    "*Similarity learning* is the area of machine learning\nfocusing on learning how similar or different two objects are.\n\nSimilarity learning is often used in areas of machine learning\nwhere there are not a fixed number of classes that objects fit into.\n[Face verification][1] is one example of such an area.\n\n[Learning-to-rank][2] is another area of machine learning where\nthe model needs to learn a similarity function between the\npieces of data in the dataset.\n\n[Distance metric learning][3] is related to similarity learning,\nbut where the similarity function is also required to obey the\nfour axioms of a [distance metric][4]. Outside of distance metric learning, similarity learning often\nlearns a pseudometric, where not all four axioms of a distance\nmetric are true.\n\n[1]: https://machinelearning.wtf/terms/face-verification/\n[2]: https://machinelearning.wtf/terms/learning-to-rank/\n[3]: https://machinelearning.wtf/terms/distance-metric-learning/\n[4]: https://machinelearning.wtf/terms/distance-metric/",
  doc2vec:
    "[doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html) is the gensim\nlibrary's name for its [paragraph vector](https://machinelearning.wtf/terms/paragraph-vector/) implementation.\ndoc2vec can be used to generate unsupervised representations of sentences, paragraphs,\nand documents.",
  resnet:
    'ResNet stands for "Residual Network" and was introduced in the paper\n[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\nResNet won the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)][1] 2015\ncompetition.\n\n[1]: http://www.image-net.org/challenges/LSVRC/',
  fasttext:
    "fastText is a project from Facebook Research for producing\n[word embeddings](https://machinelearning.wtf/terms/word-embedding/) and sentence\nclassification.\n\nThe fastText project is [hosted on Github](https://github.com/facebookresearch/fastText/) and\ninstructions for using their pre-trained word embeddings\ncan be [found here](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md).",
  "distributional similarity":
    "Distributional similarity is the idea that the meaning of words can be understood\nfrom their context.\n\nThis should not be confused with the term [distributed representation][1], which refers to the\nidea of representing information with relatively dense vectors as opposed to a one-hot\nrepresentation.\n\n[1]: https://machinelearning.wtf/terms/distributed-representation/",
  autoencoder:
    "Autoencoders are an [unsupervised learning][1] model that aim to learn\n[distributed representations][2] of data.\n\nTypically an autoencoder is a neural network trained to predict its own\ninput data. A large enough network will simply memorize the training set,\nbut there are a few things that can be done to generate useful\ndistributed representations of input data, including:\n\n1. constraining the size of the model, forcing it to learn a lower-dimensional\nrepresentation that can be used to re-construct the original\nhigher-dimensional data points.\n2. adding artificial noise to the initial data points, and training the autoencoder\nto predict the data points minus the artificial noise. See\n[denoising autoencoder][4] for more information.\n\n[1]: https://machinelearning.wtf/terms/unsupervised-learning/\n[2]: https://machinelearning.wtf/terms/distributed-representation/\n[3]: https://machinelearning.wtf/terms/neural-network/\n[4]: https://machinelearning.wtf/terms/denoising-autoencoder/",
  "learning to rank ltr":
    "Learning-to-rank is the application of machine learning\nto ranking search results, recommendations, or similar\ninformation.",
  "model parallelism":
    "Model parallelism is where multiple computing nodes evaluate\nthe same model with the same data, but using different\nparameters or hyperparameters.\n\nIn contrast to model parallelism,\n[data parallelism](https://machinelearning.wtf/terms/data-parallelism/)\nwhere the different computing nodes have the same\nparameters but different data.",
  "deep learning":
    "Deep Learning is about learning using [neural networks][1] with multiple [hidden layers][2].\n\n[1]: https://machinelearning.wtf/terms/neural-network/\n[2]: https://machinelearning.wtf/terms/hidden-layer/",
  "point estimator":
    "A point estimator estimates population parameters (e.g. mean, variance) with sample data.",
  alexnet:
    "AlexNet is a convolutional neural network architecture proposed by\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012.\n\nAt the time, it achieved state-of-the-art performance on\nthe test set for the 2010 ImageNet Large Scale Visual Recognition Competition (LSVRC). A variant of the model won the\n2012 ImageNet LSVRC with a top-5 test error rate of\n15.3%--ten percentage points ahead of the second place winner.",
  "self supervised learning":
    'Self-supervised learning is a type of\n[supervised learning](https://machinelearning.wtf/terms/supervised-learning/)\nwhere the training labels are determined by the input data.\n\n[word2vec](https://machinelearning.wtf/terms/word2vec/) and similar word embeddings are\na good example of self-supervised learning. word2vec models\npredict a word from its surrounding words (and vice versa).\nUnlike "traditional" supervised learning, the class labels\nare not separate from the input data.\n\n[Autoencoders](https://machinelearning.wtf/terms/autoencoders) are another example\nof self-supervised learning, as they are trained to\nshrink and reconstruct their inputs.',
  "dying relu":
    "*Dying ReLU* refers to a problem when training neural \nnetworks with [rectified linear units (ReLU)][1].\nThe unit dies when it only outputs 0 for any given input.\n\nWhen training with stochastic gradient descent, the unit\nis not likely to return to life, and the unit will no\nlonger be useful during training.\n\n[Leaky ReLU][2] is a variant that solves the Dying ReLU problem\nby returning a small value when the input $x$ is less than 0.\n\n[1]: https://machinelearning.wtf/terms/rectified-linear-unit-relu/\n[2]: https://machinelearning.wtf/terms/leaky-relu/",
  softmax:
    "The softmax turns $n$ numbers\nin $\\mathbb R^N$ into a probability distribution proportional\nto the size of the numbers.\n\nGiven an $n$-dimensional vector $\\mathbf v$ with all component terms\nin $\\mathbb R^N$, the softmax of $\\mathbb v$ is:\n$$\n\\mathrm{softmax}(\\mathbb v)_i =\n\\frac{\\exp{(v_i)}}\n{\\sum_{j=1}^{n} \\exp{(v_i)}}\n$$",
  "mean reciprocal rank mrr":
    "$\\newcommand{\\Correctrank}{\\mathrm{rank}}$\n\nMean Reciprocal Rank is a measure to evaluate systems that return\na ranked list of answers to queries.\n\nFor a single query, the *reciprocal rank* is\n$\\frac 1 \\Correctrank$ where $\\Correctrank$ is the position of the\nhighest-ranked answer ($1, 2, 3, \\ldots, N$ for $N$ answers returned\nin a query). If no correct answer was returned in the query, then the reciprocal\nrank is 0.\n\nFor multiple queries $Q$, the Mean Reciprocal Rank is the mean\nof the $Q$ reciprocal ranks.\n\n$$\\mathrm{MRR} = \\frac 1 Q \\sum_{i=1}^{Q} \\frac 1 {\\Correctrank_i}$$",
  "neural style transfer":
    '*Neural style transfer* refers to the use of neural networks to apply the\nstyle of a *style image* to a *content image*.\n\n(Most of the literature in neural style transfer refers to images, but recent\nresearch has explored the use of neural style transfer techniques\nto other domains.)\n\n## Examples\nThe below examples of neural style transfer are [from Justin Johnson of Stanford University][1].\nThe top left image is [Vincent van Gogh\'s The Starry Night][2] and the top right image is\na photograph of Stanford\'s Hoover tower. The bottom image is generated by Justin Johnson\nusing neural style transfer.\n\n<figure>\n<div class="cf">\n<div class="fl w-40"><img src="/images/starry_night_google.jpg" \nalt="Vincent Van Gogh\'s famous artwork titled \'The Starry Night\'"></div>\n<div class="fl w-60"><img src="/images/hoovertowernight.jpg"\nalt="A photograph of Hoover Tower on the Stanford University campus"></div>\n<div class="fl w-100"><img src="/images/starry_stanford_bigger.png"\nalt="A synthetic image generated by Justin Johnson that depicts Stanford University\'s Hoover Tower using the style of Vincent van Gogh\'s \'The Starry Night\'"></div>\n</div>\n<figcaption>\n<p><b>Top Left:</b> Vincent Van Gogh\'s famous artwork titled <em>The Starry Night</em></p>\n<p><b>Top Right:</b> A photograph of Hoover Tower on the Stanford University campus</p>\n<p><b>Bottom:</b> A synthetic image generated by Justin Johnson that depicts Stanford University\'s\nHoover Tower using the style of Vincent van Gogh\'s <em>The Starry Night</em></p>\n</figcaption>\n</figure>\n\n## History\nIn the 1990s, some computer science researchers began exploring [non-photorealistic rendering][3],\nwhich offered ways to generate images inspired by the style and texture of specific artwork\n(such as oil paintings). However, these techniques were generally limited in the types of styles\nthey could generate target images for.\n\nThis limitation began to disappear in 2015 with the publication of the paper\n[A Neural Algorithm of Artistic Style][4]. The paper\'s authors demonstrated the use\nof convolutional neural networks to generate images inspired by many different styles.\n\n## Loss Function\n\nBroadly, neural style transfer is the technique of using gradient descent\nto minimize the loss function over the following variables:\n\n - $x_c$ -- our input content image. In the above example, this would be the photograph of Stanford University\'s Hoover Tower.\n - $x_s$ -- our style image. In the above example, this would be van Gogh\'s *The Starry Night*.\n - $x$ -- our generated image. In the above example, this is the generated image of Hoover Tower in the style of *The Starry Night*.\n\nWe intend to find a value of $x$ that minimizes $\\alpha E_c(x, x_c) + \\beta E_s(x, x_s)$ where:\n\n - $E_c(\\cdot)$ is a function that compares the difference of content between $x$ and $x_c$.\n - $E_s(\\cdot)$ is a function that compares the difference of style between $x$ and $x_s$.\n - $\\alpha$ and $\\beta$ are weighting factors to balance whether the generated image favors\n accurate content or accurate style.\n\n\n[1]: https://github.com/jcjohnson/neural-style\n[2]: https://en.wikipedia.org/wiki/The_Starry_Night\n[3]: https://en.wikipedia.org/wiki/Non-photorealistic_rendering\n[4]: https://arxiv.org/abs/1508.06576',
  "no free lunch nfl theorem":
    'The "No Free Lunch" theorem is the idea that all optimizers perform equally well\nwhen averaged across all possible optimization problems.',
  backpropagation:
    "A technique to find good weight values in a neural network by trying different weights, and seeing if the change contributes positively to prediction quality.",
  "adam optimizer":
    "ADAM, or **Ada**ptive **M**oment Estimation, is a stochastic optimization\nmethod [introduced by Diederik P. Kingma and Jimmy Lei Ba][5].\n\nThey intended to combine the advantages of [Adagrad][1]'s\nhandling of sparse [gradients][3] and [RMSProp][2]'s handling\nof [non-stationary environments][4].\n\n[1]: https://machinelearning.wtf/terms/adagrad/\n[2]: https://machinelearning.wtf/terms/rmsprop/\n[3]: https://machinelearning.wtf/terms/gradient/\n[4]: https://machinelearning.wtf/terms/stationary-environment/\n[5]: https://arxiv.org/abs/1412.6980",
  "pooling layer":
    "A *pooling layer* is a common type of layer in a\n[convolutional neural network (CNN)][1]. A pooling layer does not contain\nany weights that need to be learned during neural network training.\n\nHowever, pooling layers come with two hyperparameters:\n - [Stride][7] $s$\n - [Filter (or kernel) size][8] $f$\n\nBoth of these hyperparameters have the same meaning as they do for convolutional\nlayers.\n\nPooling layers help reduce the amount of computation a convolutional neural\nnetwork needs, and can often help increase the performance of a CNN.\n\nA pooling layer will turn an $n \\times n$ matrix into a\n$\\left \\lfloor \\frac{n - f}{s} \\right ]\\rfloor + 1 \\times \\left \\lfloor \\frac{n - f}{s} \\right ]\\rfloor + 1$ matrix.\n\nThere are several types of pooling:\n - [Max pooling][2], the most common type of pooling in convolutional neural networks.\n - [Average pooling][3].\n - [Global Average Pooling (GAP)][4].\n - [$k$-Max Pooling][5] and the closely-related [Dynamic $k$-Max Pooling][6],\n which have found applications in neural networks for natural language processing.\n\n[1]: https://machinelearning.wtf/terms/convolutional-neural-network-cnn/\n[2]: https://machinelearning.wtf/terms/max-pooling/\n[3]: https://machinelearning.wtf/terms/average-pooling/\n[4]: https://machinelearning.wtf/terms/global-average-pooling-gap/\n[5]: https://machinelearning.wtf/terms/k-max-pooling/\n[6]: https://machinelearning.wtf/terms/dynamic-k-max-pooling\n[7]: https://machinelearning.wtf/terms/stride-convolution/\n[8]: https://machinelearning.wtf/terms/filter-convolution/",
  "word embedding":
    "A word embedding (or word vector) refers to a dense vector\nrepresentation of a word.\n\nBefore word embeddings, words were typically represented with\nsparse vectors in [bag-of-words](https://machinelearning.wtf/terms/bag-of-words/) models or\nwith [n-grams](https://machinelearning.wtf/terms/bag-of-n-grams/).\n\nWord embeddings are typically trained with an unsupervised model\nover a large corpus of text. In the training process, the vectors\nare updated to better predict elements of the corpus, such as\nwords surround a given target word.\n\nAt the end of this process, word embeddings often have geometric\nrelations to each other that encode semantic meaning. A common\nexample for this is using vector addition and subtraction\nto find related words. The vector\n$\\mathrm{King}_v - \\mathrm{Man}_v + \\mathrm{Woman}$ is most similar\nto the vector $\\mathrm{Queen}_v$.\nThis example comes from the [project page][1] for\n[word2vec](https://machinelearning.wtf/terms/word2vec/).\n\nCommon implementations of word embeddings include:\n\n - [word2vec](https://machinelearning.wtf/terms/word2vec/)\n - [GloVe](https://machinelearning.wtf/terms/glove-word-embeddings/)\n - [fastText](https://machinelearning.wtf/terms/fasttext/)\n\n[1]: https://code.google.com/archive/p/word2vec/",
  "jaccard index":
    "The *Jaccard index*--otherwise known as *intersection over union*--is used to calculate the similarity or difference\nof sample sets.\n\n$$\nJ(\\mathbb{A}, \\mathbb{B}) =\n\\frac{\\left | \\mathbb{A} \\cap \\mathbb{B} \\right |}\n{\\left | \\mathbb{A} \\cup \\mathbb{B} \\right |}\n$$\n\n$$\n0 \\leq J(\\mathbb{A}, \\mathbb{B}) \\leq 1\n$$\n\nThe index is defined to be 1 if the sets are empty.",
  "zero shot learning":
    "Ian Goodfellow in [a Quora answer][1] defines zero-shot learning as the following:\n\n> Zero-shot learning is being able to solve a task despite not having received any training examples of that task. For a concrete example, imagine recognizing a category of object in photos without ever having seen a photo of that kind of object before. If you've read a very detailed description of a cat, you might be able to tell what a cat is in a photograph the first time you see it.\n\n\n[1]: https://www.quora.com/What-is-zero-shot-learning",
  "bias variance tradeoff":
    "The bias-variance tradeoff refers to the problem of minimizing two different sources of error\nwhen training a supervised learning model:\n\n1. **Bias** - Bias is a consistent error, possibly from the algorithm having\nmade an incorrect assumption about the training data. Bias is often related to underfitting.\n\n2. **Variance** - Variances comes from a high sensitivity to differences in training data.\nVariance is often related to overfitting.\n\nIt is typically difficult to simultaneously minimize bias and variance.",
  "distance metric":
    'As per Wikipedia, a distance metric, metric, or distance\nfunction, "is a function that defines a distance between each pair of elements of a set."\n\nA distance metric $d(\\cdot)$ requires the following four axioms to be true\nfor all elements $x$, $y$, and $z$ in a given set.\n\n - **Non-negativity:** $d(x, y) \\geq 0$ -- The distance must always be\n greater than zero.\n - **Identity of indiscernibles:** $d(x, y) = 0 \\Leftrightarrow x = y$ -- The distance must be zero for two elements that are the same (i.e. indiscernible from each other).\n - **Symmetry:** $d(x,y) = d(y,x)$ -- The distances must be the same, no matter which order the parameters are given.\n - **Triangle inequality:** $d(x,z) \\leq d(x,y) + d(y,z)$ -- For three elements in the set, the sum of the distances for any two pairs must be greater than the distance for the remaining pair.',
  "named entity recognition in query nerq":
    "*Named Entity Recognition in Query* is a phrase used in a research paper and patent from\nMicrosoft, referring to the [Named Entity Recognition](https://machinelearning.wtf/terms/named-entity-recognition-ner/)\nproblem in web search queries.",
  hypergraph:
    "A hypergraph is a generalization of the [graph][1]. A graph has edges that connect\npairs of vertices, but a hypergraph has hyperedges that can connect any number\nof vertices.\n\n[1]: https://machinelearning.wtf/terms/graph/",
  "bag of words":
    'The phrase *bag-of-words* typically refers to a way of representing\ntext in natural language processing, although [it has been applied to computer vision][1].\n\nA bag-of-words representation contains how many times each word appears in a document,\nbut disregards the order of the words.\n\nOften, bag-of-words models will only include the $k$ most frequent words in a corpus.\nThis reduces the memory needed to store relatively-infrequent words, and the\nunderlying representation of the document is mostly the same because\ncommon words dominate the document.\n\nBag-of-words models are often highly effective at representing documents\nin tasks like classification, clustering, or topic modeling. But they\ncan struggle with tasks where word order matters, like sentiment analysis\nand machine translation. For example, in a bag-of-words model, the phrase\n*"dog bites man"* and *"man bites dog"* have identical representations.\n\n[1]: https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision',
  adadelta:
    "ADADELTA is a gradient descent-based optimization algorithm. Like [AdaGrad][1],\nADADELTA automatically tunes the learning rate.\n\n[1]: https://machinelearning.wtf/terms/adagrad/",
  "adaptive learning rate":
    'The term *adaptive learning rate* refers to variants\nof [stochastic gradient descent][1] with learning\nrates that change over the course of the algorithm\'s\nexecution.\n\nAllowing the learning rate to change dynamically\neliminates the need to pick a "good" static learning rate,\nand can lead to faster training and a trained model\nwith better performance.\n\nSome adaptive learning rate algorithms are:\n - [Adagrad][2]\n - [ADADELTA][3]\n - [ADAM][4]\n\n[1]: https://machinelearning.wtf/terms/stochastic-gradient-descent-sgd/\n[2]: https://machinelearning.wtf/terms/adagrad/\n[3]: https://machinelearning.wtf/terms/adadelta/\n[4]: https://machinelearning.wtf/terms/adam-optimizer/',
  "meteor machine translation":
    "METEOR is an automatic evaluation metric for machine translation,\ndesigned to mitigate perceived weaknesses in\n[BLEU](https://machinelearning.wtf/terms/bilingual-evaluation-understudy-bleu/). METEOR scores\nmachine translation *hypotheses* by aligning them to reference translations,\nmuch like BLEU does.",
  facet:
    "A [facet](https://machinelearning.wtf/terms/facet-plotting) can refer to a type\nof plot or chart designed to efficiently display\nmultidimensional data.\n\n[Facets](https://machinelearning.wtf/terms/facets-tool) is also the name of a tool\nfrom Google's People + AI Research (PAIR) lab designed\nto help explore datasets.",
  "decision tree":
    "A supervised learning method that iteratively refines a prediction by asking questions about the input feature most likely to affect the outcome, making a 'tree' of question branches.",
  dropout:
    'Dropout is a technique for [regularizing](https://machinelearning.wtf/terms/regularization)\nneural networks, developed by Hinton et al. and published\nin the paper [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580).\n\nThe core idea behind dropout is to randomly set some of the weights\nin a neural network to $0$ during the training phase.\n\nDropout add a hyperparameter of a "keep probability". This is\nthe probability that a weight value is left undisturbed--\nthat it will *not* be set to $0$.\n\nDropout can be considered analogous to [model-averaging](https://machinelearning.wtf/terms/model-averaging) because the process simulates training\nmany similar neural networks on the same data.',
  "one shot learning":
    "One-shot learning refers to the problem of\ntraining a statistical model (such as a\nclassifier) with only a single example per class.\n\nOne way to build a system capable of\none-shot learning is to use [representation learning][1], to learn representations or features of data\nthat can be used to accurately classify a single example.\n\n[1]: https://machinelearning.wtf/terms/representation-learning/",
  "convex combination":
    'A convex combination is a linear combination, where all\nthe coefficients are greater than 0 and sum to 1.\n\nThe [Convex combination Wikipedia article][1] gives the following example:\n\nGiven a finite number of points $x_1, x_2, \\ldots, x_n$ in a real vector\nspace, a convex combination of these points is a point of the form\n\n$$\na_1 x_1 + a_2 x _2 + \\ldots + a_n x_n\n$$\nis a convex combination if all real numbers $a_i \\geq 0$ and\n$a_1 + a_2 + \\ldots + a_n = 1$\n\n[1]: https://en.wikipedia.org/wiki/Convex_combination "Convex combination - Wikipedia"',
  "siamese neural network":
    "A *Siamese neural network* is a neural network architecture that runs two pieces of data through identical neural networks, and then the outputs are fed to a loss function measuring similarity between outputs.\n\nSiamese neural networks are a common model architecture for [one-shot learning][1].\n\nFor example, a Siamese neural network might be used to train a model to measure similarity between two different images, for the purpose of identifying whether the images are of the object.... but without training on many examples of that object.\n\n[1]: https://machinelearning.wtf/terms/one-shot-learning",
  "catastrophic forgetting":
    "Catastrophic forgetting (or catastrophic interference) is a problem\nin machine learning where a model forgets an existing learned pattern\nwhen learning a new one.\n\nThe model uses the same parameters to recognize both patterns,\nand learning the second pattern overwrites the parameters'\nconfiguration from having learned the first pattern.",
  "object localization":
    "*Object localization* is the machine learning problem\nthat encompasses [object detection][1]--finding whether\nan object exists exists in an image--and finding\nthe *location* of the object an image.\n\nThe location of an object in an image is typically represented\nas a \"bounding box\".\n\n## An example of 2-dimensional object localization\n\nBelow is a rough example of what a machine learning model\nwould output if trained and evaluated on 2D images\nfor the purpose of drawing 2D bounding boxes around a\nsingle object. In the real world, many of the exact\ndetails may vary.\n\nTo train a machine learning model to identify the presence\nor absence of a single object, the model's output would be a\nsingle 1 (for the object's presence) or 0 (for the object's absence).\nWe'll refer to this as $p_{\\mathrm{exists}}$.\n\nBut to draw a two-dimensional bounding box showing where the object\nis in the image, the model would need to process the image ane return a\nvector that looked like the following:\n\n$$\n\\begin{bmatrix}\np_{\\mathrm{exists}} \\\\\nb_{\\mathrm x} \\\\\nb_{\\mathrm y} \\\\\nb_{\\mathrm{width}} \\\\\nb_{\\mathrm{height}}\n\\end{bmatrix}\n$$\n\nIf the object exists, then the remaining values in the vector\nhave the following meanings:\n\n - $b_{\\mathrm x}$ is the X position of the object in the image. This may\n  either be the center of the object, or the X position of one of the corners.\n - $b_{\\mathrm y}$ is the Y position of the object in the image. This\n  may either be the center of the object, or the Y position of one of the corners.\n - $b_{\\mathrm{width}}$ is the width of the bounding box.\n - $b_{\\mathrm{height}}$ is the height of the bounding box.\n\nWhen the object does not exist in the given image, many machine learning models\nmay return vectors with random numbers.\n\nWhen training an object localization model, one typically uses a dataset\nwhere $p_{\\mathrm{exists}}$ is known to be 1 or 0. When evaluating an\nobject localization model, the model typically returns $p_{\\mathrm{exists}}$\nas a probability between 0 and 1. When using this probability, we may\npick a threshold--such as 0.5--where we assume the model has found an object.\n\n## Different variations in object localization\n\nIn addition to 2D images from digital cameras,\nobject localization is also relevant in other\nsensor data that might capture objects--such as a RADAR or\nLIDAR scan.\n\nObject localization models may be trained to identify multiple\nobjects, or to identify objects with 3D bounding boxes or\nnon-rectangular bounding \"boxes\".\n\n\n[1]: https://machinelearning.wtf/terms/object-detection/",
  "capture recapture model":
    'A capture-recapture model is a technique to estimate an unknown\npopulation by capturing, tagging, and re-capturing samples\nfrom the population.\n\nIn the article [How many Mechanical Turk workers are there?][1],\nPanos Ipeirotis explains a simple version of a capture-recapture\nmodel as follows:\n\n> The simplest possible technique is the following:\n>\n>  - **Capture/marking phase**: Capture $n_1$ animals, mark them, and release them back.\n>\n>  - **Recapture phase**: A few days later, capture $n_2$ animals.\n> Assuming there are $N$ animals overall, $n_1/N$ of them are marked.\n> So, for each of the $n_2$ captured animals, the probability that the\n> animal is marked is $n_1/N$ (from the capture/marking phase).\n>\n>  - **Calculation**: On expectation, we expect to see $n_2 \\cdot \\frac{n_1}{N}$\n> marked animals in the recapture phase. (Notice that we do not\n> know $N$.) So, if we actually see $m$ marked animals during the\n> recapture phase, we set $m = n_2 \\cdot \\frac{n_1}{N}$ and we get the estimate that:\n>\n> $$N=n_1 \\cdot \\frac{n_2}{m}$$\n\nHe adds that this basic version of a capture-recapture model makes\nthe following assumptions, and the estimate $N$ can be inaccurate\nwhen these assumptions are violated:\n\n> - **Assumption of no arrivals / departures ("closed population")**:\n> The vanilla capture-recapture scheme assumes that there are no\n> arrivals or departures of workers between the capture and recapture phase.\n>\n> - **Assumption of no selection bias ("equal catchability")**: The\n> vanilla capture-recapture scheme assumes that every worker in\n> the population is equally likely to be captured.\n\n[1]: http://www.behind-the-enemy-lines.com/2018/01/how-many-mechanical-turk-workers-are.html',
  "pca color augmentation":
    "The term *PCA Color Augmentation* refers to a type of\n[data augmentation][1] technique\nfirst mentioned in the paper titled\n[ImageNet Classification with Deep Convolutional\nNeural Networks][2]. This paper is famous\nfor introducing the [AlexNet][3] convolutional neural network\narchitecture, which won the 2012 ImageNet Large Scale\nVisual Recognition Competition.\n\nIn general, data augmentation is the process of increasing the size\nof a dataset by transforming it in ways that a neural network is unlikely\nto learn by itself. For example, image-recognition datasets often train\nwith images flipped vertically or horizontally.\n\nAnother form of data augmentation on image datasets is to alter\nthe color balance of the image--for example, adjusting the values of\nthe red, green, and blue pixels in the image.\n\nSpecifically, PCA Color Augmentation is designed to shift those values\nbased on which values are the most present in the image. Images with\nheavy red values and minimal green values will have their red values\naltered the most through PCA Color Augmentation.\n\nThe specific mechanism relies on the\n[principal component analysis (PCA)][4] algorithm to find the\nrelative color balance of a given image.\n\nThe AlexNet paper describes PCA Color Augmentation in this paragraph:\n\n> Specifically, we perform PCA on the set of RGB pixel values throughout the\n> ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\na Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_{xy} = \\left [ I^R_{xy}, I^G_{xy}, I^B_{xy} \\right ]^T$, we add the following quantity:\n>\n> $$\\left [ \\mathbf p_1, \\mathbf p_2, \\mathbf p_3 \\right ]\n\\left [ \\alpha_1 \\lambda_1, \\alpha_2 \\lambda_2, \\alpha_3 \\lambda_3 \\right ]^T$$\n\n[1]: https://machinelearning.wtf/terms/data-augmentation/\n[2]: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n[3]: https://machinelearning.wtf/terms/alexnet/\n[4]: https://machinelearning.wtf/terms/principal-component-analysis-pca/",
  "harmonic precision recall mean f1 score":
    'Precision answers the question, "What fraction of positive predictions are\ntrue predictions?"\n\nA cancer diagnostic test that suggested that all patients have cancer\nwould achieve \n\n$$\n\\mathrm{Precision} =\n\\frac{\\mathrm{True\\;Positives}}{\\mathrm{True\\;Positives} + \\mathrm{False\\;Positives}}\n$$\n\nRecall answers the question, "Out of all the true positives, what fraction of\nthem did we identify?"\n\nA cancer diagnostic test that suggested that all patients have cancer would achieve\nperfect recall, as all patients that *actually* have cancer would be identified.\n\n$$\n\\mathrm{Recall} =\n\\frac{\\mathrm{True\\;Positives}}{\\mathrm{True\\;Positives + False\\;Negatives}}\n$$\n\nThe F1 score is a way to combine precision and recall in the following way:\n\n$$\nF_1 = 2 * \\frac{\\mathrm{Precision} \\times \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}\n$$\n\nFor a classifier to have a high $F_1$ score, it needs to have high precision *and*\nhigh recall.',
  "one hot encoding":
    "*One-hot encoding* refers to a way of transforming data into vectors\nwhere all components are 0, except for one component with a value of 1,\ne,g.:\n$$\n0 = [1, 0, 0, 0, 0]^T\n$$\n$$\n1 = [0, 1, 0, 0, 0]^T\n$$\n$$\n\\ldots\n$$\n$$\n4 = [0, 0, 0, 0, 1]^T\n$$\nand so on.\n\nOne-hot encoding can make it easier for machine learning algorithms to\nmanipulate and learn categorical variables.",
  likelihood:
    "In statistics, likelihood is the *hypothetical probability* that a past event\nwould yield a specific outcome. Probability is concerned with the future,\nbut likelihood is concerned with the past.",
  "sequence to sequence learning seq2seq":
    "This typically refers to the method originally described by Sutskever et al. in the paper\n[Sequence to Sequence Learning with Neural Networks][1].\n\nFeedforward neural networks and many other models can learn complex patterns, but require fixed-length\ninput. This makes it difficult for these models to learn variable-length sequences. To solve this,\nthe authors applied one [LSTM](https://machinelearning.wtf/terms/long-short-term-memory-lstm/) to read the input seqeunce\nand a second LSTM to generate the output sequence.\n\nA few potential applications of sequence to sequence learning include:\n\n - Machine translation\n - Text summarization\n - Speech-to-text conversion\n\n[1]: https://arxiv.org/abs/1409.3215",
  vggnet:
    "VGGNet is a deep convolutional neural network \nfor image recognition, trained by\nthe Visual Geometry Group (VGG) at the University of Oxford.\n\nVGGNet helped the VGG team secure the [first place\nin Localization and second place in Classification][1]\nin the 2014 ImageNet Large Scale Visual Recognition Competition.\n\n[1]: http://www.image-net.org/challenges/LSVRC/2014/results#clsloc",
  "1x1 convolution":
    "A *1x1 convolution* or a *network in network* is\nan architectural technique used in some convolutional\nneural networks.\n\nThe technique was first described in the\npaper [Network In Network](https://arxiv.org/abs/1312.4400).\n\nA 1x1 convolution is a convolutional layer where the\n[filter](https://machinelearning.wtf/terms/filter-convolution/) is of dimension\n$1 \\times 1$.\n\nThe filter takes in a tensor of dimension\n$n_h \\times n_w \\times n_c$, over the $n_c$ values\nin the third dimension and\noutputting a $n_h \\times n_w$ matrix. Subsequently,\nan activation function (like [ReLU](https://machinelearning.wtf/terms/rectified-linear-unit-relu/)) is applied to the\noutput matrix.\n\nIf we have $p$ $1 \\times 1$ filters, then the output of\nthe layer is a tensor of dimension\n$n_h \\times n_w \\times p$. This is useful if\nthe number of channels $n_c$ in the previous layer of\nthe network has grown too large and needs to be \naltered to $p$ channels.\n\nThe $1 \\times 1$ convolution technique was featured\nin paper introducing the Inception network architecture,\ntitled\n[Going Deeper With Convolutions](http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf).",
  "symmetry breaking":
    'Symmetry breaking refer to a requirement of initializing machine\nlearning models like neural networks.\n\nWhen some machine learning models have weights all initialized\nto the same value, it can be difficult or impossible for the\nweights to differ as the model is trained. This is the "symmetry".\n\nInitializing the model to small random values breaks the symmetry\nand allows different weights to learn independently of each other.',
  "yolo object detection":
    '*YOLO* (an acronym standing for the phrase "You Only Look Once")\nrefers to a fast object detection algorithm. Previous attempts\nat building object detection algorithms involved running\n[object detectors][1] or [object localizers][2] multiple times over\na single image.\n\nInstead of needing multiple executions over a single image, YOLO\ndetects objects through sending an image through a single forward\npass through a [convolutional neural network][3].\n\n[1]: https://machinelearning.wtf/terms/object-detection/\n[2]: https://machinelearning.wtf/terms/object-localization/\n[3]: https://machinelearning.wtf/terms/convolutional-neural-network-cnn/',
  "bounding box":
    "A bounding box is a rectangle (in 2D datasets) or rectangular prism\n(in 3D datasets) drawn around an object identified in an image.\n\n[Object localization](https://machinelearning.wtf/terms/object-localization) is a task in computer\nvision where a model is trained to draw bounding boxes around\nobject detected in an image.",
  "anchor box":
    "*Anchor boxes* are a technique used in some [computer vision][4]\n[object detection][3] algorithms to help identify objects of different shapes.\n\nAnchor boxes are hand-picked boxes of different height/width ratios\n(for 2-dimensional boxes) designed to match the relative ratios of\nthe object classes being detected. For example, an object detector\nthat detects cars and people may have a wide anchor box to detect\ncars and a tall, narrow box to detect people.\n\nThe [Fast R-CNN][1] paper introduced the idea of using the\n[$k$-means-clustering][2] to automatically determine the appropriate\nanchor box dimensions for a given $k$ number of anchor boxes.\n\n[1]: https://machinelearning.wtf/terms/fast-r-cnn/\n[2]: https://machinelearning.wtf/terms/k-means-clustering/\n[3]: https://machinelearning.wtf/terms/object-detection/\n[4]: https://machinelearning.wtf/terms/computer-vision/",
  convolution:
    "Convolution is an operation where a *filter* (a small matrix) is applied\nto some input (typically a much larger matrix).\n\nConvolution is a common operation in image processing, but convolution\nhas also found some applications in natural language processing, audio\nprocessing, and other fields of machine learning.\n\n## Padding\n[Padding][1] is a preprocessing step before a\nconvolution operation. The input matrix is often padded to control\nthe output dimensions of the convolution, or ot preserve information\naround the edges of the input matrix.\n\n## Stride\n[Stride][2] is the number of steps the filter\ntakes in the convolution operation. \n\n## Calculating the output dimensions of a convolution\nFor example, let's say we have $n \\times n$ matrix $A$ and a\n$f \\times f$ filter $F$. The output dimension depends on two parameters -- padding $p$ and stride $s$.\n\nThe dimensions for the output matrix $A * F$ will be\n\n$$\n\\left \\lfloor \\frac{n + 2p - f}{s} \\right \\rfloor + 1 \\times\n\\left \\lfloor \\frac{n + 2p - f}{s} \\right \\rfloor + 1\n$$.\n\nIn a [same convolution][3], $s = 1$ and $p = \\frac{f - 1}{2}$.\nThe $n \\times n$ matrix $A$ gets padded to $ n + p \\times n + p$\nand the output matrix becomes $n \\times n$.\n\n[1]: https://machinelearning.wtf/terms/padding-convolution/\n[2]: https://machinelearning.wtf/terms/stride-convolution/\n[3]: https://machinelearning.wtf/terms/same-convolution/",
  "inception neural network":
    "*Inception* refers to a particular neural network model in the\nCVPR 2015 paper titled [Going Deeper With Convolutions](https://arxiv.org/abs/1409.4842).",
  "peephole connection lstm":
    'Peephole connections refer to a modification to the basic LSTM architecture.\n[WildML][1] describes LSTM peephole connections as the following:\n\n> Notably, there exist several variations on the basic LSTM architecture. A common one is creating peephole connections that allow the gates to not only depend on the previous hidden state $s_{t-1}$, but also on the previous internal state $c_{t-1}$, adding an additional term in the gate equations.\n\nPeephole connections were originally introduced by\n[Gers and Schmidhuber in 2000][2] to help LSTMs learn precise timings. From their abstract:\n\n> Surprisingly, LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps.\n\nThe paper [LSTM: A Search Space Odyssey][3] gives an overview of various types of LSTM architectures and\ntheir performance, including LSTMs with peephole connections.\n\n[1]: http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ "Recurrent Neural Network Tutorial, Part 4 - Implementing a GRU / LSTM RNN with Python and Theano"\n\n[2]: ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf "Recurrent Nets that Time and Count"\n\n[3]: https://arxiv.org/abs/1503.04069',
  "inverted dropout":
    'Inverted dropout is a variant of the original [dropout](https://machinelearning.wtf/terms/dropout)\ntechnique developed by Hinton et al.\n\nJust like traditional dropout, inverted dropout randomly\nkeeps some weights and sets others to zero. This is known\nas the "keep probability" $p$.\n\nThe one difference is that, during the training of a neural\nnetwork, inverted dropout scales the activations by\nthe inverse of the keep probability $q = 1 - p$.\n\nThis prevents network\'s activations from getting too large,\nand does not require any changes to the network during\nevaluation.\n\nIn contrast, traditional dropout requires scaling to be implemented\nduring the test phase.',
  transduction:
    "Similar to supervised learning, but does not explicitly construct a function: instead, tries to predict new outputs based on training inputs, training outputs, and new inputs.",
  "gradient descent":
    "Gradient descent is an optimization algorithm designed\nto find the minimum of a function. Many machine learning\nalgorithms use gradient descent or a variant.\n\nCommon variants include:\n - [Stochastic Gradient Descent (SGD)](https://machinelearning.wtf/terms/stochastic-gradient-descent-sgd/)\n - [Minibatch Gradient Descent](https://machinelearning.wtf/terms/minibatch-gradient-descent/)",
  accams: "ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly",
  "activation function":
    "In neural networks, an activation function defines\nthe output of a neuron.\n\nThe activation function takes the dot product of\nthe input to the neuron ($\\mathbf x$) and the weights ($\\mathbf w$).\n\nTypically activation functions are nonlinear, as that allows the\nnetwork to approximate a wider variety of functions.",
  overfitting:
    "Overfitting is when a statistical model learns patterns in the training\ndata that are too complex to generalize well.\n\n![Example of 1-dimensional overfitted data from [Wikipedia][1]](https://machinelearning.wtf/images/overfitting.png)\n\n[1]: https://en.wikipedia.org/wiki/File:Overfitted_Data.png",
  "zero padding":
    "## Signal processing\nIn signal processing, zero padding refers to the practice of adding zeroes to a time-domain\nsignal. Zero-padding is often done before performing a [fast Fourier transform][1]\non the time-domain signal.\n\n## Neural networks\nIn convolutional neural networks, zero-padding refers to surrounding a matrix with\nzeroes. This can help preserve features that exist at the edges of the original\nmatrix and control the size of the output feature map.\n\nBelow is an example of a padding operator $\\mathrm{Pad}(n, \\mathbf X)$ that\nadds $n$ layers of zeroes around the matrix $\\mathbf X$.\n$$\n\\mathbf X = \n\\begin{bmatrix}\na & b & c \\\\\nd & f & g \\\\\nh & j & k\n\\end{bmatrix},\n$$\n$$\n\\mathrm{Pad}(1, \\mathbf X) = \n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 \\\\\n0 & a & b & c & 0 \\\\\n0 & d & f & g & 0 \\\\\n0 & h & j & k & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n\n[1]: https://machinelearning.wtf/terms/fast-fourier-transform/",
  "paragraph vector":
    "*Paragraph Vectors* is the name of the model proposed by [Le and Mikolov][1] to\ngenerate unsupervised representations of sentences, paragraphs, or entire documents\nwithout losing local word order.\n\nThis is in contrast to [bag-of-words](https://machinelearning.wtf/terms/bag-of-words/) representations, which\ncan offer useful representations of documents but lose all word order information.\n\n[1]: https://arxiv.org/abs/1405.4053",
  "first order information":
    "First-order information is a term used to mean information obtained\nby computing the first derivative of a function. The first\nderivative of a function reveals the slope of a tangent\nline to the function. This gives a general idea of how the function\nis changing at that point, but does not give information\nabout the *curvature* of the function--the second derivative\nis required for that.\n\nFirst-order information should not be confused with\n[first-order logic][1].\n\n[1]: https://en.wikipedia.org/wiki/First-order_logic",
  "stationary environment":
    'A stationary environment refers to data-generating distributions\nthat do not change over time.\n\nA non-stationary environment, in contrast, refers to data-generating\ndistributions that do change over time.\n\nIt is a difficult problem to train machine learning algorithms\nto generalize well in non-stationary environments. See\n[Machine Learning in Non-Stationary Environments][1] for\nmore information.\n\n[1]: https://mitpress.mit.edu/books/machine-learning-non-stationary-environments "Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation - Mit Press"',
  "embed encode attend predict":
    "The phrase *Embed, Encode, Attend, Predict* refers to Matthew\nHonnibal's [conceptual framework for deep learning for natural\nlanguage processing][1].\n\nThe steps have the following meanings:\n\n1. **Embed** -- This is the process of turning text or\nsparse vectors into dense [word embeddigs](https://machinelearning.wtf/terms/word-embedding/).\nThese embeddings are much easier to work with than other\nrepresentations, and do an excellent job of capturing semantic\ninformation.\n\n2. **Encode** -- This is the process of encoding\na sequence of word vectors into a matrix, using\ntechniques like [recurrent neural networks](https://machinelearning.wtf/terms/recurrent-neural-network/)\nor [LSTMs](https://machinelearning.wtf/terms/long-short-term-memory-lstm/).\n\n3. **Attend** -- This refers to taking the matrix from the\n*Encode* step and transforming it into a vector, most likely\nusing an [attention mechanism](https://machinelearning.wtf/terms/attention-neural-networks/).\n\n4. **Predict** -- The final step in the Natural Language Processing\npipeline is making a prediction given the input text.\n\n\n[1]: https://explosion.ai/blog/deep-learning-formula-nlp",
  "language segmentation":
    "This phrase is most concisely described in in [this work by David Alfter][1]:\n\n> Language segmentation consists in finding the boundaries where one\n> language ends and another language begins in a text written in more than one language.\n> This is important for all natural language processing tasks.\n>\n> [...]\n>\n> One important point that has to be borne in mind is the difference between language\n> identification and language segmentation. Language identification is concerned with recognizing\n> the language at hand. It is possible to use language identification for language segmentation.\n> Indeed, by identifying the languages in a text, the segmentation is implicitly obtained.\n> Language segmentation on the other hand is only concerned with identifying language\n> boundaries. No claims about the languages involved are made.\n\n[1]: https://arxiv.org/abs/1510.01717",
  "1 bit stochastic gradient descent 1 bit sgd":
    "1-bit Stochastic Gradient Descent is a technique from Microsoft Research aimed at\nincreasing the [data parallelism][1] inherent in training deep neural networks.\nThey describe the technique in the paper\n[1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech DNNs][2].\n\nThey accelerate training neural networks with stochastic gradient descent by:\n\n1. splitting up the computation for each minibatch across many nodes in a distributed system.\n2. reducing the bandwidth requirements for communication between nodes by exchanging gradients\n(instead of model parameters) and quantizing those gradients all the way to just 1 bit.\n3. they add the quantization error from Step 2 into the next minibatch gradient before quantization.\n\n1-bit Stochastic Gradient Descent is [available is a technique][3] in [Microsoft's Cognitive Toolkit (CNTK)][4].\n\n[1]: https://machinelearning.wtf/terms/data-parallelism/\n[2]: https://www.microsoft.com/en-us/research/publication/1-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/\n[3]: https://docs.microsoft.com/en-us/cognitive-toolkit/reasons-to-switch-from-tensorflow-to-cntk\n[4]: https://github.com/Microsoft/CNTK",
  "supervised learning":
    "Supervised learning is about training on labeled data, i.e. questions with known answers. The task is to find the function that maps a set of inputs (x1, x2, x3, ...) to the value to be predicted (y).",
  "semi supervised learning":
    "Semi-supervised learning mixes labeled and labeled data\nto produce better models.\n\nIn machine learning, finding or creating correctly-labeled data\n(e.g. images with annotations and descriptions, audio\nwith text transcriptions) can be difficult or expensive.\n\nSemi-supervised learning techniques take advantage\nof a small amount of labeled data and a large amount\nof unlabeled data to produce a better model than\na purely [supervised learning](https://machinelearning.wtf/terms/supervised-learning/)\nor a purely [unsupervised learning](https://machinelearning.wtf/terms/unsupervised-learning/)\ntechnique.",
  "support vector machine svm":
    "Support Vector Machine is a classification method in supervised learning that seeks to use support vectors (cases close to the boundary) to find an optimal hyperplane separating items from different classes.",
  word2phrase:
    "word2phrase refers to a program in the\n[word2vec](https://machinelearning.wtf/terms/word2vec) toolkit that discovers\nmulti-word phrases in a corpus of words.\n\nFrom the [original word2vec Google Code page](https://code.google.com/archive/p/word2vec/):\n\n> In certain applications, it is useful to have vector representation of larger pieces of text. For example, it is desirable to have only one vector for representing 'san francisco'. This can be achieved by pre-processing the training data set to form the phrases using the word2phrase tool, as is shown in the example script ./demo-phrases.sh.",
  "top 1 error rate":
    "The term *top-1 error rate* refers method of benchmarking\nmachine learning models in the ImageNet\nLarge Scale Visual Recognition Competition.\n\nThe model is considered to have classified a given image correctly\nif the target label is the model's top prediction. This\nis in contrast to the [top-5 error rate](https://machinelearning.wtf/terms/top-5-error-rate/)\nwhere the model only needs to identify the correct label in the\nmodel's top 5 predictions.",
  "semantic hashing":
    "A *hash* is a small, lossy representation of a data point. Typically hashes\nare fixed-sized representations of variable-sized data.\n\n[Cryptographic hash functions][1] take and data and return fixed-size\nrepresentations. These typically have the property that the hash value\n(ideally) reveals no information about the original piece of data.\n\nThe [MD5 hash][2] for the string `hello` is `5d41402abc4b2a76b9719d911017c592`,\nbut the hash for `Hello` is `8b1a9953c4611296a827abf8c47804d7`. It is typically\nnot trivial to infer that these two distinct-looking hashes have similar inputs.\n\n**Semantic hashes are the opposite** -- they are fixed-length representations\nwhere similar pieces of input data should have similar hashes.\n[Word embeddings][3] are a type of semantic hash for words. Words\nthat occur in contexts end up with similar embeddings.\n\nSemantic hashes are useful for document search, information retrieval,\nor other problems where one needs to find similar pieces of data.\n\n[1]: https://simple.wikipedia.org/wiki/Cryptographic_hash_function\n[2]: https://en.wikipedia.org/wiki/MD5\n[3]: https://machinelearning.wtf/terms/word-embedding/\n[4]: https://machinelearning.wtf/terms/autoencoder/",
  "glove word embeddings":
    "GloVe, or Global Vectors, refers to a word embedding algorithm\nfrom the Stanford NLP group.",
  inceptionism:
    "*Inceptionism* refers to a visualization technique to understand what\na neural network learned. The network is fed an image,\nasked what the network detected, and then that feature in the\nimage is *amplified*. The full technique is described in the\nGoogle Research blog post titled [Inceptionism: Going Deeper into Neural Networks](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html).",
  chunking:
    "The paper [Natural Language Processing (almost) from Scratch][1] describes chunking as:\n\n> Also called shallow parsing, chunking aims at labeling segments of a sentence with syntactic constituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique tag, often encoded as a begin-chunk (e.g., B-NP) or inside-chunk tag (e.g., I-NP).\n\n[1]: https://arxiv.org/abs/1103.0398",
  "valid convolution":
    "A *valid convolution* is a type of [convolution][1] operation that does not use any [padding][2] on the input.\n\nFor an $n \\times n$ input matrix and an $f \\times f$ filter, a valid convolution\nwill return an output matrix of dimensions\n\n$$\n\\left \\lfloor \\frac{n - f}{s} \\right \\rfloor + 1 \\times\n\\left \\lfloor \\frac{n - f}{s} \\right \\rfloor + 1\n$$\n\nwhere $s$ is the [stride][3] length of the convolution.\n\nThis is in contrast to a [same convolution][4], which pads the\n$n \\times n$ input matrix such that the output matrix is also $n \n\\times n$.\n\n[1]: https://machinelearning.wtf/terms/convolution/\n[2]: https://machinelearning.wtf/terms/padding-convolution/\n[3]: https://machinelearning.wtf/terms/stride-convolution/\n[4]: https://machinelearning.wtf/terms/same-convolution/",
  textrank:
    "TextRank is a graph ranking algorithm applied to text.\nIt is useful for various unsupervised learning\ntasks in natural language processing.",
  "top 5 error rate":
    "The term *top-5 error rate* refers method of benchmarking\nmachine learning models in the ImageNet\nLarge Scale Visual Recognition Competition.\n\nThe model is considered to have classified a given image correctly\nif the target label is one of the model's top 5 predictions.",
  "test term":
    "This is the body of yet another test term. I think that this is going to be extremely promising.\n\n\n$$\n\n\\int_0^100 x^2\n\n$$",
  "stochastic block model sbm":
    "The stochastic block model is a common model for detection\nof [community structure][1]\n\n[1]: https://machinelearning.wtf/terms/community-structure/",
  adaboost:
    "AdaBoost, short for Adaptive Boosting, aka Weight Boosted Trees, uses the same training set over and over. Increases the margin (separation) like [SVMs][1].\n\n[1]: https://machinelearning.wtf/terms/support-vector-machine/",
  "distance metric learning":
    '*Distance metric learning* is the task of using a labeled dataset to learn\na [similarity learning][1] where the similarity function has\nto obey the [four axioms of a distance metric][2].\n\nThe problem is typically defined with a dataset where there some of\nthe datapoints are known to be "similar" and should be closer to each\nother than another arbitrarily-chosen datapoint in the dataset.\n\nDistance metric learning first received significant attention in\nthe machine learning community from the 2002 NIPS paper titled\n[Distance metric learning, with application to clustering with side-information][3].\n\n## Axioms of a distance metric\n\nThe four axioms of a distance metric are:\n\n - **Non-negativity:** $d(x, y) \\geq 0$ -- The distance must always be\n greater than zero.\n - **Identity of indiscernibles:** $d(x, y) = 0 \\Leftrightarrow x = y$ -- The distance must be zero for two elements that are the same (i.e. indiscernible from each other).\n - **Symmetry:** $d(x,y) = d(y,x)$ -- The distances must be the same, no matter which order the parameters are given.\n - **Triangle inequality:** $d(x,z) \\leq d(x,y) + d(y,z)$ -- For three elements in the set, the sum of the distances for any two pairs must be greater than the distance for the remaining pair.\n\n[1]: https://machinelearning.wtf/terms/similarity-learning/\n[2]: https://machinelearning.wtf/terms/distance-metric/\n[3]: http://ai.stanford.edu/~ang/papers/nips02-metric.pdf',
  synset:
    "A synset is [WordNet][1]'s terminology for a [synonym ring][2].\n\nWordNet is a database of English words grouped into sets of synonyms.\nWordNet's synsets are often useful in information retrieval and natural\nlangauge processing tasks to discover when two different words can mean\nsimilar things.\n\n[1]: https://wordnet.princeton.edu/\n[2]: https://en.wikipedia.org/wiki/Synonym_ring",
  "random initialization":
    "Random initialization refers to the practice of using random numbers\nto initialize the weights of a machine learning model.\n\nRandom initialization is one way of performing [symmetry breaking](https://machinelearning.wtf/terms/symmetry-breaking), which is the act of preventing all of\nthe weights in the machine learning model from being the same.",
  "recurrent neural network language model rnnlm":
    "A Recurrent Neural Network Language Model (RNNLM) is\na recurrent neural network tasked with modeling\nlanguages.",
  "multinomial mixture model mmm":
    'A multinomial mixture model is a mixture of multinomial distributions.\n\nThe Wikipedia page for the [multinomial distribution][1] notes the following regarding the relationship between the multinomial distribution and the categorical distribution:\n\n> Note that, in some fields, such as natural language processing, the categorical and multinomial distributions are conflated, and it is common to speak of a "multinomial distribution" when a categorical distribution is actually meant. This stems from the fact that it is sometimes convenient to express the outcome of a categorical distribution as a "1-of-$K$" vector (a vector with one element containing a 1 and all other elements containing a 0) rather than as an integer in the range ${\\displaystyle 1\\dots K} 1 \\dots K$; in this form, a categorical distribution is equivalent to a multinomial distribution over a single trial.\n\nThis is important to remember when reading about [categorical mixture models](https://machinelearning.wtf/terms/categorical-mixture-model-cmm/) versus multinomial mixture models.\n\n[1]: https://en.wikipedia.org/wiki/Multinomial_distribution',
  "long short term memory lstm":
    "Long short term memory (LSTM) networks try to reduce the vanishing and exploding gradient problem during the backpropagation in recurrent neural networks. LSTM are in general, a RNN where each neuron has a memory cell and three gates: input, output and forget. The purpose of the memory cell is to retain information previously used by the RNN, or forget if needed. LSTMs are explicitly designed to avoid the long-term dependency problem in RNNs, and have been shown to be able to learn complex sequences better than simple RNNs.\n\nThe structure of a memory cell is: an input gate, that determines how much of information from the previous layer gets stored in the cell; the output gate, that determines how of the next layer gets to know about the state of the current cell; and the forget gate, which determines what to forget about the current state of the memory cell.",
  "unsupervised learning":
    "Unsupervised learning is about problems where we don't have\nlabeled answers, such as clustering, dimensionality reduction, and anomaly detection.",
  "face recognition":
    "Face recognition is the problem of identifying whether an input\nimage contains the faces of any of $k$ people... or if the image has\nnone of the $k$ faces.\n\nFace recognition is a harder problem than [face verification][1]\nbecause face verification only compares a single image to one person,\nwhereas face recognition does this for $k$ people.\n\n[1]: https://machinelearning.wtf/terms/face-verification/",
  perplexity:
    "Wikipedia [defines perplexity][1] as the following:\n\n> In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n\n[1]: https://en.wikipedia.org/wiki/Perplexity",
  "latent dirichlet allocation differential evolution ldade":
    "LDADE is a tool proposed by Agrawal et al. in a paper\ntitled [What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)](https://arxiv.org/abs/1608.08176).\n\nIt tunes [LDA](https://machinelearning.wtf/terms/latent-dirichlet-allocation-lda) parameters\nusing\n[Differential Evolution](https://machinelearning.wtf/terms/differential-evolution/) to increase\nthe [clustering stability](https://machinelearning.wtf/terms/clustering-stability/) of standard LDA.",
  "provenance tracking":
    'In data acquisition, provenance tracking is keeping track of\n"where each piece of data comes from and whether it is still up-to-date".\nThis quoted definition comes from Philip Guo [in an article for the ACM][1]\nwhere he elaborates:\n\n> The main problem that programmers face in data acquisition is keeping track of provenance, i.e., where each piece of data comes from and whether it is still up-to-date.  It is important to accurately track provenance, since data often needs to be re-acquired in the future to run updated experiments.  Re-acquisition can occur either when the original data sources get updated or when researchers want to test alternate hypotheses.  Also, provenance can enable downstream analysis errors to be traced back to the original data sources.\n\nIn [a data science tutorial][2], Andrew Davison writes about the history of the term:\n\n> The term comes originally from the art world, where it refers to the chronology of the ownership and/or location of a work of art.\n> Having detailed evidence of provenance can help to establish that a painting has not been altered or stolen, and is not a forgery. Wikipedia has a nice entry on the provenance of the Arnolfini Portrait by van Eyck.\n> More recently, the term has been applied to other fields, including archaeology, palaeontology, and science more generally, where it refers to having knowledge of all the steps involved in producing a scientific result, such as a figure, from experiment design through acquisition of raw data, and all the subsequent steps of data selection, analysis and visualization. Such information is necessary for reproduction of a given result, and can serve to establish precedence (in case of patents, Nobel prizes, etc.)\n\n[1]: https://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext\n\n[2]: http://rrcns.readthedocs.io/en/latest/provenance_tracking.html',
  "second order information":
    "The term *second-order information* refers to information\nabout a function gained by computing its second derivative.\nThe second derivative reveals information about the function's\ncurvature.",
  "facets tool":
    "Facets is a a plotting and visualizaton tool created by \nthe People + AI Research (PAIR) initiative at Google.\n\nFacets is broken into two tools with the following goals:\n\n - **Facets Overview** -- summarize statistics for features collected from datasets\n - **Facets Dive** -- explore the relationship between different features in a dataset\n\nFrom the Facets homepage, they state that\n\n> Success stories of (Facets) Dive include the detection of classifier failure, identification of systematic errors, evaluating ground truth and potential new signals for ranking.",
  "stride convolution":
    "In convolutions, the *stride* is the number of horizontal\nand vertical steps that the filter takes over the original matrix.",
  "weight sharing":
    "In neural networks, weight sharing is a way to reduce the number of parameters while allowing\nfor more robust feature detection. Reducing the number of parameters can be\nconsidered a form of [model compression](https://machinelearning.wtf/terms/model-compression/).",
  margin:
    "In machine learning, a *margin* often refers to the\ndistance between the two hyperplanes that separate linearly-separable classes of data points.\n\n![In this [image from Wikipedia][1], the dotted lines represent the two hyperplanes dividing the white and black data points. The region between the lines is the margin.](https://machinelearning.wtf/images/margin.png)\n\nThe term is most commonly used when discussing\n[support vector machines][2], but often appears in\nother literature discussing boundaries between points in a vector space.\n\n[1]: https://en.wikipedia.org/wiki/Support_vector_machine#Hard-margin\n[2]: https://machinelearning.wtf/terms/support-vector-machine-svm/",
  "facet plotting":
    "In statistical plotting, a facet is a type of plot. Data\nis split into subsets and the subsets are plotted\nin a row or grid of subplots.\n\nThe term is common among users of [ggplot2](http://ggplot2.org/),\na plotting package for the\n[R statistical computing language](https://www.r-project.org/about.html).\n\n[Facet](https://machinelearning.wtf/terms/facets-tool) is also the name of a plotting and\nvisualizaton tool created by the People + AI Research (PAIR)\ninitiative at Google. \n\n![This is a facet wrap as generated by the R package `ggplot2`. This image comes from [Plotting multiple groups with facets in ggplot2][1].](https://machinelearning.wtf/images/faceting.png)\n\n[1]: https://www3.nd.edu/~steve/computing_with_data/13_Facets/facets.html",
  variance:
    "Wikipedia describes variance as follows:\n\n> In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean, and it informally measures how far a set of (random) numbers are spread out from their mean. \n\nVariance the square of standard deviation, or rather, [standard deviation][1] is the square root of variance.\nThus, sometimes variance is written as $\\sigma^2$ where $\\sigma$ stands for the standard deviation.\n\n[1]: https://machinelearning.wtf/terms/standard-deviation/",
  "recurrent neural network":
    "Recurrent neural networks (RNN) are feed-forward neural networks, but differently than traditional feed-forward models, RNNs contain an internal memory. A RNN have an internal loop that allows information to persist in the network. Neurons receive information not just from the previous layer, but also from themselves from the previous pass. This means that the order of inputs to the RNN matter, and may give different results with different order, as RNNs have state.\n\nRNNs are particularly sensitive to the vanishing and exploding gradient problems, where depending on the activation functions used, the information can get lost over time. [Long short-term memory networks][1](LSTMs) addresses this problem. RNNs are commonly used with sequential data, due to the fact that an RNN is sensitive to the order of the inputs, like in the natural language processing area. Common examples of sequential data includes texts, audio and video.\n\n[1]: https://machinelearning.wtf/terms/long-short-term-memory-lstm/",
  "sobel filter convolution":
    "The Sobel filter is a set of two convolution filters used to detect horizontal\nand vertical edges in images.\n\nThe horizontal filter is\n\n$$\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n2 & 0 & -2 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n$$\n\nand the vertical filter is\n\n$$\n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1\n\\end{bmatrix}\n$$",
  termite:
    "Termite is a visual analysis tool to determine the quality of topic models\nlike [latent Dirichlet allocation](https://machinelearning.wtf/terms/latent-dirichlet-allocation-lda/).\n\nTermite lays out document terms as a table of circles where:\n\n - rows represent document terms\n - columns represent topics\n - circular areas represent term probabilities",
  "dimensionality reduction":
    "Dimensionality reduction is about taking a set of data, and reducing its number of dimensions in such a way as to balance information size and independence of the features' information. The point is to get a smaller dataset that still retains most of the original information.",
  sense2vec:
    'sense2vec refers to a system in a paper titled\n[sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings](https://arxiv.org/abs/1511.06388).\nIt solves a problem with previous word embeddings like [word2vec](https://machinelearning.wtf/terms/word2vec) and [GloVe](https://machinelearning.wtf/terms/glove-word-embeddings/)\nwhere words of different senses (e.g. "duck" as an animal and "duck" as a verb) are represented by the\nsame embedding.\n\nsense2vec uses word sense information to train more accurate word embeddings.',
  "convex hull":
    "The convex hull of a set $X$ in an affine space over the reals is the smallest\n[convex set][1] that contains $X$. When the points are two dimensional,\nthe convex hull can be thought of as the rubber band around the points of $X$.\n\nAs per Wikipedia, a [convex set][1] is the smallest affine space\nclosed under convex combination.\n\nA [convex combination][2] is a linear combination where\nall the coefficients are greater than 0 and all sum to 1.\n\n[1]: https://en.wikipedia.org/wiki/Convex_set\n[2]: https://machinelearning.wtf/terms/convex-combination/",
  "multiple crops at test time":
    "*Multi-crop at test time* is a form of data augmentation that a model uses\nduring test time, as opposed to most data augmentation techniques\nthat run during training time.\n\nBroadly, the technique involves:\n\n - cropping a test image in multiple ways\n - using the model to classify these cropped variants of the test image\n - averaging the results of the model's many predictions\n\nMulti-crop at test time is a technique that some machine learning researchers\nuse to improve accuracy at test time. The technique\nfound popularity among some competitors in the\nImageNet Large Scale Visual Recognition Competition\nafter the famous AlexNet paper, titled\n[ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), used\nthe technique.",
  "triplet loss":
    "*Triplet loss* is a loss function that come from the paper [FaceNet: A Unified Embedding for Face Recognition and Clustering][1]. The loss function\nis designed to optimize a neural network that produces\nembeddings used for comparison.\n\nThe loss function operates on triplets, which are three examples from the dataset:\n\n - $x_i^a$ -- an *anchor* example. In the context of FaceNet, $x_i^a$ is a photograph of a person's face.\n - $x_i^p$ -- a *positive* example that has the same identity as the anchor. In FaceNet, this is a second picture of the same person as the picture from the anchor example.\n  - $x_i^n$ -- a *negative* example that represents a different entity. For FaceNet, this would be an image of a second person--someone different than the person represented by the anchor and positive examples.\n\nThe triplet loss function is designed to train the model to produce embeddings such that the positive\nexample $x_i^p$ is closer to the anchor than the negative example $x_i^n$.\n\n## Math Details\n\nMore formally, for an embedding function $f(x) \\in \\mathbb R^d$ that embeds input data $x$ into\na $d$-dimensional vector, we want\n\n$$\n||f(x_i^a) - f(x_i^p)||_2^2 + \\alpha \\leq\n||f(x_i^a) - f(x_i^n)||_2^2\n$$\n\nfor all $N$ possible triplets of $x_i^a$, $x_i^p$, and $x_i^n$. The $||x||_2^2$ operator is the square of the Eucledian norm. The $\\alpha$ symbol stands for a\n[margin][2] to ensure that the model doesn't\nmake the embeddings $f(x_i^a)$, $f(x_i^p)$, and\n$f(x_i^n)$ equal each other to trivially satisfy\nthe above inequality.\n\nThis leads to the following loss function\nover the $N$ possible triplets.\n\n$$\n\\sum_i^N \\left [\n||f(x_i^a) - f(x_i^p)||_2^2 -\n||f(x_i^a) - f(x_i^n)||_2^2 + \\alpha\n\\right ]_+\n$$\n\nthe $[x]_+$ operator [stands for][3] $\\max(0,x)$.\n\n## Triplet selection\n\nIn a typical dataset, many triplets of $x_i^a$, $x_i^p$, and $x_i^n$ will satisfy the inequality\nin the previous section without the algorithm\nlearning a useful embedding. This slows down the\ntraining of a machine learning algorithm that uses\nthe triplet loss function.\n\nTo speed training back up, it makes sense to train\nthe algorithm on examples where $f(x_i^a)$ is\ncloser to $f(x_i^n)$ than $f(x_i^p)$ in the embedding\nspace (ignoring the term $\\alpha$).\n\n[1]: https://arxiv.org/abs/1503.03832\n[2]: https://machinelearning.wtf/terms/margin/\n[3]: https://math.stackexchange.com/questions/215211/meaning-of-mathematical-operator-that-consists-of-square-brackets-with-a-plus-si",
  gradient:
    "The gradient is the vector generalization of the derivative.\n\nFor a function $f([x_1, \\ldots, x_n]^T)$, the gradient $\\nabla_x f([x_1, \\ldots, x_n]^T)$\nis the vector containing the $n$ partial derivatives of $f$ with respect to each $x_i$.",
  "data augmentation":
    "*Data augmentation* is the process of using computer algorthms\nor other synthetic means to increase the size of a collected dataset.\n\nMachine learning algorithms typically become more resistant\nto [overfitting](https://machinelearning.wtf/terms/overfitting) when they are trained with more data.\nBut in many cases, it can be expensive to collect more data.\n\nIt is often possible to significantly increase the size of a dataset\nby computing simple transformations that are unlikely to be learned\nby the model, but also do not change the value of the labels.\n\nFor example, if one were building a model to identify pictures\nof cats versus non-cats, flipping the pictures horizontally or vertically\nincreases the size of the available image data. When the pictures\nare flipped, whether they contain a cat or not doesn't change.\nDespite this, many statistical models would be unlikely to learn\nflip-invariance without a significant amount of data--either collected\nfrom the real-world or generated through data augmentation.",
  boosting:
    "Learners trained serially so that instances on which the preceding base learners are not accurate are given more emphasis in training later base-learners; actively tries to generate complementary learners, instead of leaving this to chance.",
  "independent identically distributed iid":
    "A collection of random variables is independent and identically distributed\nif they have these properties:\n\n1. they all have the same probability distribution.\n2. they are all mutually independent of each other.",
  "mode collapse":
    "*Mode collapse*, also known as the *Helvetica scenario*, is a common\nproblme when training [generative adversarial networks][1].\n\n[1]: https://machinelearning.wtf/terms/generative-adversarial-network-gan/",
  bagging:
    "Bagging, short for bootstrap aggregating, is training different base learners on different subsets of the training set randomly, by drawing random training sets from the given sample (with replacement).",
  "community detection":
    "Community detection refers to the problem of detecting whether\na graph has [community structure][1].\n\n[1]: https://machinelearning.wtf/terms/community-structure/",
  "neural checklist model":
    "Neural checklist models were introduced in the paper [Globally Coherent Text Generation with Neural Checklist Models](https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf) by Kiddon et al.\n\nA neural checklist model is a recurrent neural network that tracks an agenda of text strings that should be mentioned in the output.\n\nThis technique allows the neural checklist model to generate *globally coherent* text, as opposed to text from traditional RNNs that is only locally coherent.\n\nThe original paper describes applying the neural checklist model\nto recipes and dialogue responses for information systems,\nwhere there already exists a pre-existing notion of all\nthe topics that should be present in a natural language response.",
  "data parallelism":
    "Data parallelism is when data is distributed across multiple\nnodes in a distributed computing environment, and then\neach node acts on the data in parallel.\n\nOn each node, the computation is the same, but the data is\ndifferent.",
  "beam search":
    "Beam search is a memory-restricted version of breadth-first search.\n\nBeam search commonly appears in machine translation or other literature where\n[sequence-to-sequence](https://machinelearning.wtf/terms/sequence-to-sequence-learning-seq2seq/) learning is common.\nIn this domain, beam search allows the neural network to consider many candidate\nresponses instead of selecting the highest-scoring token at each step.\n\nGoogle's [blog post announcing SyntaxNet][1] explains the advantages of beam search\nfor sequence-to-sequence learning in greater detail:\n> At each point in processing many decisions may be possible\u2014due to ambiguity\u2014and a neural network gives scores for competing decisions based on their plausibility. For this reason, it is very important to use beam search in the model. Instead of simply taking the first-best decision at each point, multiple partial hypotheses are kept at each step, with hypotheses only being discarded when there are several other higher-ranked hypotheses under consideration.\n\n[1]: https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html",
  "non max suppression":
    "Non-max suppression refers to the idea of suppressing predicted\ninformation that is not predicted with the highest confidence.\n\nNon-max suppression is commonly used in computer vision software,\nwhere a computer vision model may identify many nearby redundant edges\nor [bounding boxes](https://machinelearning.wtf/term/bounding-box).\n\nTo reduce this redundancy, only the maximum feature in a set\nof related features are kept. For example, a computer vision model\nmight identify a single object using multiple overlapping bounding boxes.\nThe bounding box with the highest prediction probability is kept,\nand the *overlapping* boxes are removed in favor of the kept box.\nOther boxes may be unaffected by non-max suppression if they are too far away.\n\nIn the case of bounding boxes, non-max suppression is said to\nprune low-confidence bounding boxes that have a high\n[intersection over union](https://machinelearning.wtf/terms/jaccard-index/)\nwith the highest-confidence bounding box they intersect with.",
  "he initialization":
    'The term *He initialization* refers to the first author in the paper\n"[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)".\n\nHe initialization initializes the bias vectors of a neural network\nto $0$ and the weights to random numbers drawn from a Gaussian\ndistribution where the mean is $0$ and the variance is\n$\\sqrt(2/n_l)$ where $n_l$ is the dimension of the previous layer.',
  "distributed representation":
    'In machine learning, data with a *local representation* typically has 1 unit per element.\nA 5-word vocabulary might be defined by a 5-dimensional vector, with\n$[1, 0, 0, 0, 0]^T$ denoting the first word, $[0, 1, 0, 0, 0]^T$ denoting the second word,\nand so forth.\n\nDistributed representations are the opposite, instead of concentrating the meaning\nof a data point into one component or one "element", the meaning of the\ndata is distributed across the whole vector.\n\nThe word that is $[1, 0, 0, 0, 0]^T$ in a local representation might look like\n$[-0.150, -0.024, -0.233, -0.253, -0.183]^T$ in a distributed representation.',
  "bag of n grams":
    "A bag-of-$n$-grams model is a way to represent a document,\nsimilar to a [bag-of-words][/terms/bag-of-words/] model.\n\nA bag-of-$n$-grams model represents a text document as\nan unordered collection of its $n$-grams.\n\nFor example, let's use the following phrase and divide\nit into bi-grams ($n = 2$).\n\n> James is the best person ever.\n\nbecomes\n\n - `<start>James`\n - `James is`\n - `is the`\n - `the best`\n - `person ever.`\n - `ever.<end>`\n\nIn a typical bag-of-$n$-grams model, these 6 bigrams would be\na sample from a large number of bigrams observed in a corpus.\nAnd then *James is the best person ever.* would be encoded\nin a representation showing which of the corpus's bigrams\nwere observed in the sentence.\n\nA bag-of-$n$-grams model has the simplicity of the bag-of-words\nmodel, but allows the preservation of more word locality\ninformation.",
  "computer vision":
    "Computer vision is the field of teaching computers to perceive sensor\ndata--such as from cameras, RADAR, and LIDAR sensors--to achieve\nan understanding of what is in the data.\n\nComputer vision is a wide-ranging field that comprises of many techniques\nand subfields.",
  "face detection":
    "*Face detection* is the problem of detecting wheter an image has\na (usually human) face in it.\n\nThe problem of identifying whether the image has a specific\n*single* person's face is called [face verification][1]. The problem\nof identifying whether the image has any of $k$ person's faces\nis called [face recognition][2].\n\n[1]: https://machinelearning.wtf/terms/face-verification/\n[2]: https://machinelearning.wtf/terms/face-recognition/",
  "leaky relu":
    "Leaky ReLU is a type of [activation function][1] that tries\nto solve the [Dying ReLU problem][2].\n\nA traditional rectified linear unit $f(x)$ returns 0 when $x \\leq 0$.\nThe *Dying ReLU problem* refers to when the unit gets stuck this\nway--always returning 0 for any input.\n\nLeaky ReLU aims to fix this by returning a small, negative,\nnon-zero value instead of 0, as such:\n\n$$\nf(x) = \n\\begin{cases}\n\\max(0,x) & x > 0 \\\\\n\\alpha x & x \\leq 0\n\\end{cases}\n$$\nwhere $\\alpha$ is typically a small value like $\\alpha = 0.0001$.\n\n[1]: https://machinelearning.wtf/terms/activation-function/\n[2]: https://machinelearning.wtf/terms/dying-relu/",
  "tree lstm":
    "Tree-LSTMs are a variant of Long Short Term Memory (LSTM) neural networks.\n\nA traditional LSTM is structured as a linear chain, and displays\nstrong performance on sequence modeling tasks--such as machine translation.\n\nHowever, some types of data (such as text) are better represented as\ntree structures instead of sequences. Thus, Tree-LSTMs were\n[introduced by Tai, et al][1] in 2015.\n\n[1]: https://arxiv.org/abs/1503.00075",
  word2vec:
    "`word2vec` refers to a pair of models, open-source software, and pre-trained word embeddings\nfrom Google.\n\nThe models are:\n\n - [skip-gram](https://machinelearning.wtf/terms/skip-gram/), using a word to predict the surrounding $n$ words\n - [continuous-bag-of-words (CBOW)](https://machinelearning.wtf/terms/continuous-bag-of-words-cbow), using the context of the surrounding\n $n$ words to predict the center word.\n\nThe original paper is titled [Efficient Estimation of Word Representations in\nVector Space](https://arxiv.org/abs/1301.3781) by Mikolov et al.\n\nThe source code was originally hosted on\n[Google Code](https://code.google.com/p/word2vec) but is now\nlocated [on Github](https://github.com/tmikolov/word2vec).",
  adversarial:
    "Typically referring to attacks or cherry picking inputs to fool neural networks",
  model:
    "In AI/ML, a model replicates a decision process to enable automation and understanding. AI/ML models are mathematical algorithms that are trained using data and human expert input to replicate a decision an expert would make when provided that same information.",
  resolution: "Resolution refers to the number of pixels in an image.",
  "single shot detector":
    "A deep learning architecture containing a backbone model and a single shot head, refers to models which require one passthrough of an image to create detections.",
  backbone: "A base model template used for machine learning.",
  pyramid:
    "A representation of data where sub-representations of multiple parts of an ML model are used to run predictions",
  "ap ":
    "Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved.",
  precision:
    "The number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search",
  recall:
    "Recall is the number of relevant documents retrieved by a search divided by the total number of existing relevant documents",
  segmentation:
    "Segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images.",
  "P-R curves": "Precision-Recall curves.",
  training:
    "The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. ",
  pretrain:
    "A process where training on a larger more general dataset is performed prior to training on a new usecase.",
  autoregressive:
    "An autoregressive (AR) model predicts future behavior based on past behavior.",
  parameter:
    "A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. They are required by the model when making predictions. ",
  mobilenet: "A type of neural network optimized for small size and speed.",
  cnn:
    "In deep learning, a convolutional neural network is a class of deep neural networks, most commonly applied to analyzing visual imagery.",
  "Convolutional":
    "In deep learning, a convolutional neural network is a class of deep neural networks, most commonly applied to analyzing visual imagery.",
  latency:
    "The delay before a transfer of data begins following an instruction for its transfer.",
};
export const dictionary = new Map(Object.entries(terms));
